{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:38:44.589976Z",
     "start_time": "2019-12-07T12:38:37.853888Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle as p\n",
    "import numpy as np\n",
    "from plotly import graph_objects as go\n",
    "import os\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from tqdm.notebook import tqdm  # last tqdm update\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:38:46.968885Z",
     "start_time": "2019-12-07T12:38:46.333076Z"
    }
   },
   "outputs": [],
   "source": [
    "first_ten = []\n",
    "for file in os.listdir('./Adam'):\n",
    "    with open('./Adam/' + file, 'rb') as inp:\n",
    "        try:\n",
    "            first_ten.append(p.load(inp)[0][:10])\n",
    "        except:\n",
    "            pass\n",
    "first_ten = np.array(first_ten)\n",
    "first_ten.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Распределение градиентов\n",
    "\n",
    "## Данные\n",
    "\n",
    "Для первых ста итераций обучения были получены значения градиентов для весов между десятью нейронами на последнем слое и\n",
    "Предпоследним скрытым слоем с 512 нейронами.\n",
    "\n",
    "## Первый взгляд\n",
    "\n",
    "Давайте посмотрим на все градиенты для каждого нейрона за все итерации. Такая апроксимация не слишком математична, так как для каждого веса распределение свое, однако такая интерперетация более наглядна. Полученная величина -- смесь нормальных распределений. Оценим эту смесь через `GaussianMixture` и построим гистограммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:39:08.143241Z",
     "start_time": "2019-12-07T12:38:54.257679Z"
    }
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "\n",
    "data = []\n",
    "all_grad = []\n",
    "mixt_est = []\n",
    "\n",
    "\n",
    "for param in range(10):\n",
    "    param_grad = first_ten[:, param, :].reshape(-1)\n",
    "    all_grad.append(param_grad)\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "\n",
    "for it in range(10):\n",
    "    data.append(go.Histogram({\n",
    "        'x': all_grad[it],\n",
    "        'opacity': 0.7,\n",
    "        'histnorm': 'probability density',\n",
    "        'name': 'param: {} all weights'.format(it+1)\n",
    "    }))\n",
    "    cur_model = GaussianMixture(5)\n",
    "    cur_model.fit(all_grad[it].reshape(-1, 1))\n",
    "    x = np.linspace(np.min(all_grad[it]), np.max(all_grad[it]), 200).reshape(-1, 1)\n",
    "    y = cur_model.score_samples(x).reshape(-1)\n",
    "    if it == 0:\n",
    "        mixt_est.append(go.Scatter({\n",
    "            'x': x.reshape(-1),\n",
    "            'y': np.exp(y),\n",
    "            'marker': {\n",
    "                'color': 'red'\n",
    "            },\n",
    "        \n",
    "            'name': 'Gaussian mixture estimator'\n",
    "        }))\n",
    "        continue\n",
    "        \n",
    "    mixt_est.append(go.Scatter({\n",
    "        'x': x.reshape(-1),\n",
    "        'y': np.exp(y),\n",
    "        'marker': {\n",
    "            'color': 'red'\n",
    "        },\n",
    "        \n",
    "        'showlegend': False\n",
    "    }))\n",
    "    \n",
    "\n",
    "fig = make_subplots(cols=4, rows=3)\n",
    "\n",
    "for idx, trace in enumerate(data):\n",
    "    fig.add_trace(mixt_est[idx], col=idx//3 + 1, row=idx%3+1)\n",
    "    fig.add_trace(trace, col=idx//3 + 1, row=idx%3+1)\n",
    "    \n",
    "    \n",
    "layout = go.Layout({\n",
    "    'title': 'Distribution of gradients',\n",
    "    'width': 1000,\n",
    "    'height': 700,\n",
    "    'template': 'plotly_white',\n",
    "    #'legend_orientation': 'h',\n",
    "    'legend': {\n",
    "        'x': 0.8,\n",
    "        'y': 0.4\n",
    "    }\n",
    "})\n",
    "fig.update_layout(layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "\n",
    "Как видим данные вполне неплохо подходят под данное распределение, а значит мы можем продолжить нашу проверку на нормальность градиентов каждого веса.\n",
    "\n",
    "## Критерий согласия Шапиро-Уилка\n",
    "\n",
    "Для каждого веса для каждого нейрона проверим гипотезу $H_0: \\text{\"Градиенты на данном весе распределены нормально\"} $ против альтернативы $H_1: \\text{\"Иначе\"} $. Для этого воспользуемся критерием согласия Шапиро-Уилка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:39:08.684713Z",
     "start_time": "2019-12-07T12:39:08.532201Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "p_values = []\n",
    "for param in tqdm(range(10)):\n",
    "    p_values_param = []\n",
    "    for weight in range(512):\n",
    "        p_value = shapiro(first_ten[:, param, weight])[1]\n",
    "        p_values_param.append(p_value)\n",
    "    p_values.append(p_values_param)\n",
    "\n",
    "p_values = np.array(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:39:09.040456Z",
     "start_time": "2019-12-07T12:39:09.035948Z"
    }
   },
   "outputs": [],
   "source": [
    "p_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, число не отвергнутых гипотез и отвергнутых гипотез можем видеть в выводе следующе ячейки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:39:09.395439Z",
     "start_time": "2019-12-07T12:39:09.389488Z"
    }
   },
   "outputs": [],
   "source": [
    "accepted = p_values[p_values > alpha]\n",
    "rejected = p_values[p_values <= alpha]\n",
    "len(accepted), len(rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же посмотрим на каждый нейрон и веса, принадлежащие конкретному нейрону."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:39:09.949631Z",
     "start_time": "2019-12-07T12:39:09.939553Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, param_p in enumerate(p_values):\n",
    "    accepted = len(param_p[param_p > alpha])\n",
    "    rejected = len(param_p[param_p <= alpha])\n",
    "    print('For parameter {} Shapiro-Wilk test results are:\\n accepted {}; rejected {}; percentage accepted {:.2f}'.format(\n",
    "    idx+1, accepted, rejected, accepted/len(param_p)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Множественная проверка гипотез\n",
    "\n",
    "Воспользуемся поправками Холма и Бенджамини-Хохберга. Посмотрим какая из них даст более мощный результат и воспользуемся такой.\n",
    "\n",
    "Будем проверять гипотезу $H_0: \\text{\"В целом градиенты распределены нормально\"} $ против $ H_1: \\text{\"Иначе\"} $\n",
    "\n",
    "### Метод Холма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:39:10.569630Z",
     "start_time": "2019-12-07T12:39:10.488396Z"
    }
   },
   "outputs": [],
   "source": [
    "reject, p_corrected, a1, a2 = multipletests(p_values.reshape(-1), \n",
    "                                            alpha = 0.05, \n",
    "                                            method = 'holm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:39:10.938407Z",
     "start_time": "2019-12-07T12:39:10.931698Z"
    }
   },
   "outputs": [],
   "source": [
    "reject.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод Бенджамини-Хохберга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:39:12.622555Z",
     "start_time": "2019-12-07T12:39:12.616420Z"
    }
   },
   "outputs": [],
   "source": [
    "reject, p_corrected, a1, a2 = multipletests(p_values.reshape(-1), \n",
    "                                            alpha = 0.05, \n",
    "                                            method = 'fdr_bh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:39:13.501311Z",
     "start_time": "2019-12-07T12:39:13.492378Z"
    }
   },
   "outputs": [],
   "source": [
    "reject.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод \n",
    "С помощью Метод Бенджамини-Хохберга удалось отвергнуть больше гипотез, а значит он является более мощным в данном случае."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подсчет дисперсии learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим градиент, для которого гипотеза о нормальности не отвергается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:39:16.695271Z",
     "start_time": "2019-12-07T12:39:16.684619Z"
    }
   },
   "outputs": [],
   "source": [
    "norm_gradients = first_ten[:, ~reject.reshape(10, 512)]\n",
    "norm_gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:43:26.276260Z",
     "start_time": "2019-12-07T12:43:26.261287Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "grad_number = np.random.randint(1, norm_gradients.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:43:26.693865Z",
     "start_time": "2019-12-07T12:43:26.687818Z"
    }
   },
   "outputs": [],
   "source": [
    "grad_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T18:03:27.647495Z",
     "start_time": "2019-12-05T18:03:27.641178Z"
    }
   },
   "source": [
    "grad_number = 309, 774 — большие отличия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:43:31.051220Z",
     "start_time": "2019-12-07T12:43:30.638326Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', font_scale=1.3, palette='Set3')\n",
    "grad_sample = norm_gradients[:, grad_number]\n",
    "\n",
    "sns.distplot(grad_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:43:35.602303Z",
     "start_time": "2019-12-07T12:43:35.592213Z"
    }
   },
   "outputs": [],
   "source": [
    "def adaptive_learning_rate(gradients, beta=0.999):\n",
    "    coefs = beta ** np.arange(len(gradients), 0, -1)\n",
    "    sum_ = np.sum(coefs*gradients**2)\n",
    "    coef = np.sqrt( (1 - beta**len(gradients)) / ( (1 - beta) * sum_ ) )\n",
    "    return coef\n",
    "\n",
    "\n",
    "def rho_t(t, beta=0.999):\n",
    "    rho_inf = 2 / (1 - beta) - 1\n",
    "    rho = rho_inf - 2 * t * (beta**t) / (1 - beta**t)\n",
    "    return rho\n",
    "\n",
    "\n",
    "def var_psi(sigma, t):\n",
    "    return rho_t(t) / (2 * (rho_t(t) - 2) * (rho_t(t) - 4) * sigma**2)\n",
    "\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:43:36.064463Z",
     "start_time": "2019-12-07T12:43:36.054125Z"
    }
   },
   "outputs": [],
   "source": [
    "def psi_variance_plot(rejected=True):\n",
    "    if rejected:\n",
    "        norm_gradients = first_ten[:, reject.reshape(10, 512)]\n",
    "    else:\n",
    "        norm_gradients = first_ten[:, ~reject.reshape(10, 512)]\n",
    "        \n",
    "    grad_number = np.random.randint(1, norm_gradients.shape[1])\n",
    "    grad_sample = norm_gradients[:, grad_number]\n",
    "    \n",
    "    from sklearn.utils import resample\n",
    "    N_samples = 1000\n",
    "    alpha = 0.95\n",
    "\n",
    "    psi_var = []\n",
    "    for i in tqdm(range(1, len(grad_sample) + 1)):\n",
    "        grad_slice = grad_sample[:i]\n",
    "        bootstrap_samples = np.array([resample(grad_slice) \n",
    "                                      for i in range(N_samples)])\n",
    "        psi_bootstrap = [adaptive_learning_rate(cur_sample) \n",
    "                         for cur_sample in bootstrap_samples]\n",
    "        psi_var.append(np.var(psi_bootstrap))\n",
    "    \n",
    "    x = np.linspace(0, 100, 101)\n",
    "    real = go.Scatter({\n",
    "        'x': x[5:],\n",
    "        'y': psi_var[5:],\n",
    "        'name': 'Real variance'\n",
    "    })\n",
    "\n",
    "    theor = go.Scatter({\n",
    "        'x': x[5:],\n",
    "        'y': var_psi(np.std(grad_sample), x[5:]),\n",
    "        'name': 'Theoretical variance'\n",
    "    })\n",
    "\n",
    "\n",
    "    return [real, theor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала сравним реальную и теоретическую дисперсиию $\\psi(g_1, \\dots, g_t)$ в зависимости от $t$ для случая, когда гипотеза о нормальности отверглась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:43:42.407370Z",
     "start_time": "2019-12-07T12:43:37.063443Z"
    }
   },
   "outputs": [],
   "source": [
    "layout = go.Layout({\n",
    "    'title': 'Estimated variance',\n",
    "    'xaxis': {\n",
    "        'title': 'iteration'\n",
    "    },\n",
    "    'yaxis': {\n",
    "        'title': 'variance'\n",
    "    },\n",
    "    'template': 'plotly_white',\n",
    "    'width': 700,\n",
    "    'height': 500,\n",
    "})\n",
    "go.Figure(data=psi_variance_plot(), layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим такой же график для случая, когда гипотеза о нормальности не отвергается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:44:29.994225Z",
     "start_time": "2019-12-07T12:44:24.897124Z"
    }
   },
   "outputs": [],
   "source": [
    "go.Figure(data=psi_variance_plot(rejected=False), layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что на первых итерациях дисперсия градиентов, распределение которых отлично от нормального, в основном выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:45:15.581295Z",
     "start_time": "2019-12-07T12:45:15.017157Z"
    }
   },
   "outputs": [],
   "source": [
    "first_ten_radam = []\n",
    "for file in os.listdir('./RAdam'):\n",
    "    with open('./RAdam/' + file, 'rb') as inp:\n",
    "        try:\n",
    "            first_ten_radam.append(p.load(inp)[0][:10])\n",
    "        except:\n",
    "            pass\n",
    "first_ten_radam = np.array(first_ten_radam)\n",
    "first_ten_radam.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка гипотез методом Холма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:45:16.962975Z",
     "start_time": "2019-12-07T12:45:16.800892Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "p_values_radam = []\n",
    "for param in tqdm(range(10)):\n",
    "    p_values_param = []\n",
    "    for weight in range(512):\n",
    "        p_value = shapiro(first_ten[:, param, weight])[1]\n",
    "        p_values_param.append(p_value)\n",
    "    p_values_radam.append(p_values_param)\n",
    "\n",
    "p_values_radam = np.array(p_values_radam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:45:18.872224Z",
     "start_time": "2019-12-07T12:45:18.698846Z"
    }
   },
   "outputs": [],
   "source": [
    "reject_radam, p_corrected, a1, a2 = multipletests(p_values.reshape(-1), \n",
    "                                            alpha = 0.05, \n",
    "                                            method = 'holm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка дисперсии для RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:45:21.092048Z",
     "start_time": "2019-12-07T12:45:21.084842Z"
    }
   },
   "outputs": [],
   "source": [
    "def rectified_psi(gradients, beta=0.999):\n",
    "    t = len(gradients)\n",
    "    rho_inf = 2 / (1 - beta) - 1\n",
    "    rho = rho_t(t)\n",
    "    r_t = np.sqrt((rho - 4) * (rho - 2) * rho_inf / ((rho_inf - 4) * (rho_inf - 2) * rho))\n",
    "    return r_t * adaptive_learning_rate(gradients, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:45:27.214685Z",
     "start_time": "2019-12-07T12:45:27.199674Z"
    }
   },
   "outputs": [],
   "source": [
    "def rectified_psi_variance_plot(rejected=True):\n",
    "    if rejected:\n",
    "        norm_gradients = first_ten_radam[:, reject_radam.reshape(10, 512)]\n",
    "    else:\n",
    "        norm_gradients = first_ten_radam[:, ~reject_radam.reshape(10, 512)]\n",
    "        \n",
    "    grad_number = np.random.randint(1, norm_gradients.shape[1])\n",
    "    grad_sample = norm_gradients[:, grad_number]\n",
    "    \n",
    "    from sklearn.utils import resample\n",
    "    N_samples = 1000\n",
    "    alpha = 0.95\n",
    "\n",
    "    psi_var = []\n",
    "    for i in tqdm(range(1, len(grad_sample) + 1)):\n",
    "        grad_slice = grad_sample[:i]\n",
    "        bootstrap_samples = np.array([resample(grad_slice) \n",
    "                                      for i in range(N_samples)])\n",
    "        psi_bootstrap = [rectified_psi(cur_sample) \n",
    "                         for cur_sample in bootstrap_samples]\n",
    "        psi_var.append(np.var(psi_bootstrap))\n",
    "    \n",
    "    x = np.linspace(0, 100, 101)\n",
    "    real = go.Scatter({\n",
    "        'x': x[5:],\n",
    "        'y': psi_var[5:],\n",
    "        'name': 'Real variance'\n",
    "    })\n",
    "    \n",
    "    return real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим аналогичный график для `rectified_psi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:45:43.085577Z",
     "start_time": "2019-12-07T12:45:36.795021Z"
    }
   },
   "outputs": [],
   "source": [
    "go.Figure(data=[rectified_psi_variance_plot()], layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T12:45:57.356602Z",
     "start_time": "2019-12-07T12:45:51.863512Z"
    }
   },
   "outputs": [],
   "source": [
    "go.Figure(data=[rectified_psi_variance_plot(rejected=False)], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы можем заметить, RAdam понижает дисперсию learning rate для нормальных и отличных от нормального весов примерно одинаково."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # AdamWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T20:30:13.626732Z",
     "start_time": "2019-12-05T20:30:12.336581Z"
    }
   },
   "outputs": [],
   "source": [
    "first_ten_w = []\n",
    "for file in os.listdir('./AdamW'):\n",
    "    with open('./AdamW/' + file, 'rb') as inp:\n",
    "        try:\n",
    "            first_ten_w.append(p.load(inp)[0][:10])\n",
    "        except:\n",
    "            pass\n",
    "first_ten_w = np.array(first_ten_w)\n",
    "first_ten_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "p_values_w = []\n",
    "for param in tqdm(range(10)):\n",
    "    p_values_param = []\n",
    "    for weight in range(512):\n",
    "        p_value = shapiro(first_ten[:, param, weight])[1]\n",
    "        p_values_param.append(p_value)\n",
    "    p_values_w.append(p_values_param)\n",
    "\n",
    "p_values_w = np.array(p_values_w)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
