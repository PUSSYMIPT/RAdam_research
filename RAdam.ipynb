{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RAdam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUSSYMIPT/RAdam_research/blob/master/RAdam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXV9U-lez6gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.optim.optimizer import Optimizer, required"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OTv3okTm10x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorboardcolab import TensorBoardColab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kipQuLeFtFvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC3m-5uMIW4j",
        "colab_type": "text"
      },
      "source": [
        "# Реализация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaPAV0PXIJXJ",
        "colab_type": "text"
      },
      "source": [
        "## Adam Warmup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1vtvcXe2zap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdamW(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup = 0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        \n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, warmup = warmup)\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(AdamW, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "                \n",
        "                if group['warmup'] > state['step']:\n",
        "                    scheduled_lr = 1e-8 + state['step'] * group['lr'] / group['warmup']\n",
        "                else:\n",
        "                    scheduled_lr = group['lr']\n",
        "\n",
        "                step_size = scheduled_lr * math.sqrt(bias_correction2) / bias_correction1\n",
        "                \n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * scheduled_lr, p_data_fp32)\n",
        "\n",
        "                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_piiAMOJITyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RAdam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHegYKkj2Kxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class RAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        \n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
        "            for param in params:\n",
        "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
        "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = group['buffer'][int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    elif self.degenerated_to_sgd:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = -1\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif step_size > 0:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ-wvqA12Ozx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PlainRAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "                    \n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "\n",
        "        super(PlainRAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(PlainRAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                beta2_t = beta2 ** state['step']\n",
        "                N_sma_max = 2 / (1 - beta2) - 1\n",
        "                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif self.degenerated_to_sgd:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
        "                    p_data_fp32.add_(-step_size, exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cBnnpQQg9l0",
        "colab_type": "code",
        "outputId": "4d624f6e-6839-48ba-fbb4-67f1e9053b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.get_device_name(0), torch.cuda.device_count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Tesla K80', 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGDueMEZSsBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr3WacCZZMJ8",
        "colab_type": "text"
      },
      "source": [
        "# Cifar\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CJMT32iZSXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffZrFnD8TVRT",
        "colab_type": "code",
        "outputId": "21f7c670-1117-4622-864c-8d654aef91ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "     ]\n",
        "     )\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:07, 24130954.52it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfcLm3mHTZ4v",
        "colab_type": "code",
        "outputId": "f9ead657-f75f-4369-dc73-dfcf5a44b1e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAB5CAYAAAAgYXpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  dog horse plane horse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_xGcJcrTgt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqoesEtFbh0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g17zlo6cYOw",
        "colab_type": "code",
        "outputId": "43b87dc6-2123-4551-9f2f-8e0c1ac44350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mruZ8tbGcoa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "class Config:  \n",
        "  def __init__(self, **kwargs):\n",
        "    for key, value in kwargs.items():\n",
        "      setattr(self, key, value)\n",
        "\n",
        "\n",
        "model_config = Config(\n",
        "    cuda = True if torch.cuda.is_available() else False,\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    seed = 2,\n",
        "    lr = 0.01,\n",
        "    epochs = 4,\n",
        "    save_model = False,\n",
        "    batch_size = 32,\n",
        "    log_interval = 100\n",
        ")\n",
        "    \n",
        "class Trainer:\n",
        "  \n",
        "  def __init__(self, config, train_loader, test_loader):\n",
        "    \n",
        "    self.cuda = config.cuda\n",
        "    self.device = config.device\n",
        "    self.seed = config.seed\n",
        "    self.lr = config.lr\n",
        "    self.epochs = config.epochs\n",
        "    self.save_model = config.save_model\n",
        "    self.batch_size = config.batch_size\n",
        "    self.log_interval = config.log_interval\n",
        "    \n",
        "    self.globaliter = 0\n",
        "    self.tb = TensorBoardColab()\n",
        "    \n",
        "    torch.manual_seed(self.seed)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if self.cuda else {}\n",
        "\n",
        "    self.train_loader = train_loader\n",
        "\n",
        "    self.test_loader = test_loader\n",
        "\n",
        "\n",
        "    self.model = Net().to(self.device)\n",
        "    self.optimizer = config.optimizer(self.model.parameters(), lr=self.lr)\n",
        "      \n",
        "\n",
        "  def change_conf(self, config):\n",
        "    self.cuda = config.cuda\n",
        "    self.device = config.device\n",
        "    self.seed = config.seed\n",
        "    self.lr = config.lr\n",
        "    self.epochs = config.epochs\n",
        "    self.save_model = config.save_model\n",
        "    self.batch_size = config.batch_size\n",
        "    self.log_interval = config.log_interval\n",
        "    self.optimizer = config.optimizer(self.model.parameters(), lr=self.lr)\n",
        "      \n",
        "  def train(self, epoch):\n",
        "  \n",
        "    self.model.train()\n",
        "    for batch_idx, (data, target) in enumerate(self.train_loader):\n",
        "      \n",
        "      self.globaliter += 1\n",
        "      data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      predictions = self.model(data)\n",
        "\n",
        "      \n",
        "      loss = criterion(predictions, target)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      if batch_idx % self.log_interval == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, batch_idx * len(data), len(self.train_loader.dataset),\n",
        "                  100. * batch_idx / len(self.train_loader), loss.item()))\n",
        "        self.tb.save_value('Train Loss', 'train_loss', self.globaliter, loss.item())\n",
        "\n",
        "\n",
        "  def test(self, epoch):\n",
        "    self.model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data, target in self.test_loader:\n",
        "        data, target = data.to(self.device), target.to(self.device)\n",
        "        predictions = self.model(data)\n",
        "\n",
        "        test_loss += F.nll_loss(predictions, target, reduction='sum').item()\n",
        "        prediction = predictions.argmax(dim=1, keepdim=True)\n",
        "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
        "\n",
        "      test_loss /= len(self.test_loader.dataset)\n",
        "      accuracy = 100. * correct / len(self.test_loader.dataset)\n",
        "\n",
        "      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "          test_loss, correct, len(self.test_loader.dataset), accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDP743mSwbp7",
        "colab_type": "code",
        "outputId": "a6e3db3f-fa89-4ca6-fc00-e1a1fdba9440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "cifar_config = Config(\n",
        "    cuda = True if torch.cuda.is_available() else False,\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    seed = 2,\n",
        "    lr = 0.0001,\n",
        "    epochs = 10,\n",
        "    save_model = False,\n",
        "    batch_size = 32,\n",
        "    log_interval = 1000,\n",
        "    optimizer = RAdam\n",
        ")\n",
        "cifar_trainer = Trainer(cifar_config, trainloader, testloader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://c868fe2a.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FDOiaz_xLOP",
        "colab_type": "code",
        "outputId": "e8a81140-e46a-4c67-ef30-73f59881645b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "for epoch in range(1, cifar_trainer.epochs + 1):\n",
        "    cifar_trainer.train(epoch)\n",
        "    cifar_trainer.test(epoch)\n",
        "    cifar_trainer.tb.flush_line('train_loss')\n",
        "\n",
        "if (cifar_trainer.save_model):\n",
        "    torch.save(trainer.model.state_dict(),\"mnist_cnn.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.947324\n",
            "Train Epoch: 1 [4000/50000 (8%)]\tLoss: 0.502278\n",
            "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 0.599281\n",
            "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 0.636181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkfkx-wHdES_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}