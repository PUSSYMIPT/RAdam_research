{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RAdam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUSSYMIPT/RAdam_research/blob/master/RAdam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXV9U-lez6gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.optim.optimizer import Optimizer, required"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OTv3okTm10x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorboardcolab import TensorBoard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kipQuLeFtFvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "4b33f2e0-1c90-4cda-f0c1-0c9698c928cc"
      },
      "source": [
        ""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-930c698a457b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorBoardColab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: get_writer() missing 1 required positional argument: 'self'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC3m-5uMIW4j",
        "colab_type": "text"
      },
      "source": [
        "# Реализация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaPAV0PXIJXJ",
        "colab_type": "text"
      },
      "source": [
        "## Adam Warmup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1vtvcXe2zap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdamW(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup = 0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        \n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, warmup = warmup)\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(AdamW, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "                \n",
        "                if group['warmup'] > state['step']:\n",
        "                    scheduled_lr = 1e-8 + state['step'] * group['lr'] / group['warmup']\n",
        "                else:\n",
        "                    scheduled_lr = group['lr']\n",
        "\n",
        "                step_size = scheduled_lr * math.sqrt(bias_correction2) / bias_correction1\n",
        "                \n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * scheduled_lr, p_data_fp32)\n",
        "\n",
        "                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_piiAMOJITyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RAdam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHegYKkj2Kxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        \n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
        "            for param in params:\n",
        "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
        "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = group['buffer'][int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    elif self.degenerated_to_sgd:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = -1\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif step_size > 0:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ-wvqA12Ozx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PlainRAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "                    \n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "\n",
        "        super(PlainRAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(PlainRAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                beta2_t = beta2 ** state['step']\n",
        "                N_sma_max = 2 / (1 - beta2) - 1\n",
        "                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif self.degenerated_to_sgd:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
        "                    p_data_fp32.add_(-step_size, exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cBnnpQQg9l0",
        "colab_type": "code",
        "outputId": "827fe5d0-6d89-4960-9714-aeb6454a3fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.get_device_name(0), torch.cuda.device_count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Tesla K80', 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGDueMEZSsBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr3WacCZZMJ8",
        "colab_type": "text"
      },
      "source": [
        "# Cifar\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CJMT32iZSXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffZrFnD8TVRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8a754d05-24d3-4c05-fb25-2c5936e634b0"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "     ]\n",
        "     )\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfcLm3mHTZ4v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "6eee114d-4c0b-4321-f08a-44145f892c2a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAB5CAYAAAAgYXpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO19e3gc1Xn37+yyWbGRWCQUiUW2IqMI\nVBvX4JiLwTEmhsRACLQhAdIQ0lygF9q05ekX8tEkps3TLzRJ07QptASSOk24xdwDIYBjcAyOQRiM\nLwgLYSFrI0uspaylrHdZds/3x/ueOe9qR6uVZMtacn7P42fG58ycOTNnNPt770prDQcHBweHykPg\ncE/AwcHBwWFqcB9wBwcHhwqF+4A7ODg4VCjcB9zBwcGhQuE+4A4ODg4VCvcBd3BwcKhQTOsDrpRa\npZR6VSn1mlLq+oM1KQcHBweHiaGm6geulAoC2AXgPAB9AJ4HcIXWeufBm56Dg4ODw3g4Yhrnngbg\nNa316wCglLoLwMUAxv2ARyIRffTRR0/jkg4ODg6/f+jv709ord8ztn06H/AmAHvE//sAnF7qhKOP\nPhpXX331NC7p4ODg8PuHG2+88Q2/9kNuxFRKXa2U6lBKdaRSqUN9OQcHB4ffG0znAx4HMFf8fw63\nFUBrfavWeonWekkkEpnG5RwcHBwcJKbzAX8eQJtSap5S6l0ALgfw0MGZloODg4PDRJiyDlxr/bZS\n6loAvwAQBPADrfWOyY6zevXqqU7hkGNjJ22fe/YFr+0zn30/AODmNW8BAM676F1e3+l1Mze3sc+t\n3OcY3/2Wt19bR3OPRA/WrCoPfs/tE9d+BQAQDge9tqoq2gYE5QlxdykWFBT7OXPepGdpkS/jmNwE\n/fkxWwDI8UmZDG1TKTtKMpkEACSSQ15bfz8J23v6ur22YH9vwXXevHJ1WZMKiocU4ocTmM0RKsGC\nzYTITbQgY/Du21aXfex0jJjQWj8K4NHpjOHg4ODgMDVM6wP+TsLax38LAPjud3/ktW18dA0A4LzL\nLvPakjFi4P94AzGQr3yn1uuLLSQXyaWn2nE/uIy2qxbbttaDN+1xkOZtVVFP1lAsAOks0Z0I1CGf\nUSUiLyiqH4vK8jZYgoq9LfYNqcz6HThJlGLiE7F0cy9ZMZEsn5Rh5j0yMuL1JUeIgQ/zVu4PJW3b\nWB+3GmHyMteUj8o8jwIGPpZ5i77ZxspLrbuE9+6IhZnO+knMskfi4ODg4FAu3AfcwcHBoULxe6VC\nibNU+J//9lMAwP33POb1dXbvpp2gUDuESQY8MGKtfN/6gdmbRxsrQaL/SbrAfevTXtt9N7NAFLW/\nlW1n1AMALlhGqov5LXaMVax+aS7nhsbDABuTovW2rYosrC3tNdMZ+fcCIbZOhoS10YjL5Yrxfod5\nKgOfvlyJPj/4GUeN6C2vbdqkCsiohqSKKM/qlHSa3t2MULVlMtSZlm3ZDG/HVwhFxPPLBYrnbeYp\njbreMzcH+hkAfR5SqedW5hD+8FlIr8lnkIJnb1RVOZ9OvznxeMPlzm3iIR0cHBwcZive8Qz8+m/c\n4e1/93u3AQDSQwkAQH2d9ftrmkcsOxgWlhc28h3TaDnCEibeGxLcIINLU+bcajEGGwgF2+l6hefz\nG27oftN2RmkerfOse+JnTqDtFVehPAyTu1c2nvCaQg1ttDOnKJ2CwxiUYtmStZY6znRNxiA1Gfix\nSr/peC6DYiKGNEvynEqleZsq2AJAKk37B9L2hDSfbJi4HyIF1NpnjtmCLuof66LnR9kFcnxfwUDx\nYX4SCcb0TYRSbHtC8Dllu41O4WVxDNzBwcGhQuE+4A4ODg4Vine8CuU/br7N20+zTNPavgAAEK0S\nwg07oKYDVo5JJqmtqdkaAx/4Cu/wdlBca8M2Gn+nDU5DB8em5oQI9szzPP5WFoRDQq3BA3ZvsxGT\nX9n3YwDA6ztsssfmUs7kAbqvQM6Kt7me7QCAYN76raP55BKDHEz8RuwbNdPMpRXu37YLABBbeEJZ\nx5fr3zsWfsZDP5QbmGfezrRoC47ZyvGM+V0en/Vpy2SpBkAubdUkI2l6V0bZUDmatmekfYyYWW7L\nZcXdjNEVhOX/fay0oTFbwKql8j76D19VSKC4LzidUNeiC5R32CSDLf0vNQU67Ri4g4ODQ4XiHc/A\n6+sbvf1kln6vwlXEQmV2xCwz71zeMo8gs/JIZPw0uA1i/9KFvLNQNF4y/tw6mY48sdG2Pbqett2/\nFhamx2ie6ZGiZI/+aCSJIRPv8Joi1czPRgQX6x2lbfOy8sadMo47xOMXI73tJW8/Vt8yqXO9qMFg\ncVvIh93lfNil3xjloCA/CW/lJTMjJJmtvfVGr22wm6Sr+adfBABY8vHP2zHC5jw7RjpjDJWjXtso\nGzGT3CaNmKNpassIxp5hhp6VRsxQuOBeqgU99Ii6oKpVfux5jIhRwGzNGqAYeRmxOXYsHwTKpMyG\nFZebX8aPsfsEYiLAf95Tlfa8caZ3uoODg4PD4YL7gDs4ODhUKN7xKpRI0Ip1uSzJK1WsOglEbNRl\nmGXjgPThZm1DJFteJaHxU0j5o91shQbjr7z9d4sjLy86d/XqZ8cfOPpeAEAiaRNFZhIkQwdCdnaj\nnIhobjMZ+erOkY7m05TtDjpk8W229KZtelNUxWi7m9RMwYhYx9jkDKb5EoarzPhuzwV+4UY0LjXW\n2HOAMRK4WYK0vffbb6SShE/ctcZra2C79OYnSf/Ws3uX13fx5/8eABCqsjEPI6weGU3JhFWFqpMC\nP3DeTxe00fHZgkjMQhWKLN/i2STFaxX0oY+BEq+dMU5GhD4jxZdPimlUl1E3JlSmcdIzkpb75+Cj\n3zGPKC/mGPIZb7JpZwHHwB0cHBwqFhMycKXUDwB8BMCg1vokbqsDcDeAFgA9AD6htZ5MCP+MIZCz\nP7VVbGSp5hAxSdICTAey8jeNs/gHy4ylenAbbS9bWPq4ctC5+TVvv/30901pjOZly739H3/nnwEA\nHc9bw+a+/gEAwCkLyK1y5W4budnUTjdRv7jFDuiRd0kfTL+JHH1L9Bm6KvOvmKjTftFmHliptLYi\n6UyCzu3v3Gmbki/SbOYvpSu2zisaQbp8/vTpXUX9Blm2uOUEJQr5WS+Lz7S7wWKKZ5+afccC7PLp\nl/qjisd49H//zWt79H+JeUfFu9vFjzKxjxj1qzd+0+tbv34dAGDVVdd7ba0L5gMAUjk73+QoPd8D\nzKzTwo3QMPBMxjLwvDH2B8enjZLlml2ZLrYcViufoud2KBqTvKhZwbpDkeJzvWsW7ZTGZBluwOei\n5jalrHKwmHM54/wPgFVj2q4HsE5r3QZgHf/fwcHBwWEGMSED11pvUEq1jGm+GMAK3l8D4CkAXzqI\n8zpoyApOYwILqiP0m1hTFRB91JYXv5P5ajo3GylPAXYwmLdBpPog1DmrX+DtrryQ/Bnjg5b5xurJ\nCTLM3Gbn1q1e34YOYuqRR+y9xxMUoXTpJy/12uaf/ce0w4EgGJX+agdoGxG6UbPfu8e2NbI7W/TM\n8e9ld6e329P1KgCga7eNmDr2BApyqplXzLwNtnVanfkv1z0CAFjoQ2FkMQODYFn+XoJjBYqz9BnG\nmc+FRBtNIMg8LST82557jAK4nvrpLV5bA8eUiToKGNxXePUVK0VFkQaK+Hpu09P2mhzAForaoK79\nKRKgU6wXT6dF0A4rcVPCjdC6D46vTK4S5NwUjAjJx8Jt5QZAeceJMXI8pXqReDNUji55sslQ/MYs\ns0CDeXP8MhVOaU4CU2XyjVpr8yXYC6Cx1MEODg4ODgcf01bFaK01Cl0ECqCUulop1aGU6pCWbQcH\nBweH6WGqboQDSqmY1rpfKRVDoX2oAFrrWwHcCgDHHXfcuB/6Q4W6eismHsPpY+e3twAoTHdZxSqU\nnHAZzOVpP+DnF5jkW4kemnqSzQsObtrX2KkrAACtm5702uLbugAA+94gYWpPj60q3s/iclC4Wn74\no2QKOXGezcmS7nwdgHXNROxYe9FaeqYjndu8pj1xusaxsZjXVmesTvgtb4Xb3wgZGzdv2uQ17e3j\nMZpt2YuF51yI8bB1B6lOqoUtNZBjHYQPhdk/ysZcIeYGSob18VbIxUYVF/Cx1OULcp/S863mc7s3\nP+F1PXjz12knNeC1NTTQTdRHrZqnsYne6zM+8zUAwAmLz/L6cpwe2RgpAWAwQeqSwaQ1WidHqC3D\nIZtZoULxij0Il0HPfTA3PgcscNXjwwtcLX3ynYw1FRcYMflRZoX6KMN/h2FhAK8ao4rwc88rN1+K\nl652gvH8YFxIzfG+hTYmcC2cCFNl4A8BME7DVwF4cIrjODg4ODhMEeW4Ed4JMljWK6X6AHwNwDcA\n3KOU+hyANwB84lBOslxI+8gP730ZAJBK7ffazvoAGXc+/aefAgDsi1vGeRTT8cUL53ht//mDuwAA\n+/2sC4eIeR8ycMbDU+Zb9nzf924HADzWQxThgDjceKmd1mJpa+u8JQCAYMQy32CYmWCjCUuyhSgA\nYr41c1q8lrAhdtJIGzFSUrGok+qnMbbv6LJzq6HZLTlzfNYtEWD2+dwj93ltiY0c5HTOBUXHDw0V\nC5RBv6gT2wsAyOWKLXRByZFMfg9BQ0NMBXdu3gAA2HzPrfbwFM1bujBm2MjYErNWu7YPU+6TyBJy\nGx0Sqspkkt5/GbQzykE7SWEJHU7Sc04ZV0HhfusJGAVFIbLiLv0R8EuLKPPFmCZZum5M3TlJSs3H\nShpwq8M0g5A4cuxH7UjxWpn5yiSKpe7BTE2uvvc8/KrJyUAlw7x9LlAiHmxSKMcL5YpxulYepDk4\nODg4OEwBLhLTwcHBoULxjsiF8v5LvgoA2PLgP5U8rrqGfq+SIySD3f7Nb3h9oWqSfd4asf7Jl5x7\nDgBg7ZO2en2lo7XZFjVYvozUKa/3UF4Vadc54ijaxpqssbE+xuql+mZxpJGTpeqEkOwj3+1QlQ2T\ni7VQbc7Bwb1eW3CADGhRU6NU5GuJNNK1pP9SdZjUOsHm8Qs0bHz8FW//zu/eBADY17Pea9uzk9Vn\nPiqU4WEy7mWzQo3Aag8/W6Y1avlEXxbUdKQxpLjfv3MLAGD7z8jnGwnr2x6pIp/5Y+qsuqmKnZxr\nj7YqlAyPO5wg1U82I1PB0vqMyPSwSaNCsX7xw6xmMvlOgmKOJk+QfEGMvTbnq0fgeQk9gbF5yoBW\no0kqsAma3CNmWKH+YG0JwuKEaJTmKbOw5EyQKN/CfmsDttGfQoM3/h0INYhs9FGJeKoWcc9mN1tm\njdVsqYmMA8fAHRwcHCoUFcvAv3WHzWUxEfM2aG1tAQDc/s2/4xb7U57lYMBtO3Z7bQsXUFRfODOF\nn8aDiOTzRCGip04jXir9O9oIRnPFNZ8DAMTqyIg4NGTdynb2E0Ntbre125qamZYX8GFiwz0bfw0A\n+OEdd3o9wTA9t49c8iGvrZ7dOtPimY6CHn68k1wF4z19Xt95f0yGytZWO49IZOIo1a1bbM6Xmx+l\n/CEiWA8JjI/+AY5RE/QoGBhjXQNg3p+sd5x9n8zhkoGbvCfZ3h6vbfsjdwAAIlysIyRcEb2SZ8IY\neHwzvZOtC9q8tm7O2RNP0HuSy1grX4oryR8QdDjFkaZDw/Y4Y9g0VeZDwrKYM4UafFL45Uv41Mmw\nD49ti8NHU+QCGK2zDgEmWtocJgtRsEck6kXeEyMg1goX0T1jqt3X+rwunWLc/ZnCOfpCRl2WYOWi\nHoxn5Mwa98diezaykoG7kmoODg4Ovz9wH3AHBweHCkXFqVCMVPb3f3KZaI2M6bVomX+2t9+126Rv\nMQJasVD9etzWnTQqlPqQPO4gwkiwMncS2wk377BNezaTSuHSaahQOrY8BwDo3mYLcJ6zmLJvnXUh\nqSniImFUVTfJnS0LF3ltKU54FBWRgcleMrrN+8DS8S8u1AIrllPFioaYvZcsy9UbN20GAFzz5a96\nfd/rpUSXK1fa8dtPt2lyx8P2l7cWtZVSm0gMDsZ52kIlwrqQgOA8eZabveOkRYrl94CQy40/8L5n\nH/fashxVmmIxXiokGpto2zLPFmOIVJGMHqm2Ko5h1lX0jtD7HRShikZVlRGRlSOjdPwBURNz5Hfs\n181rFRJ6m1yY9oPvsvfn3bL0fY+1QGLtT2/z9ps48lam6N3T3wMAWHKqXc+5MVINDSdpDXpEwrLl\np5MqLtdvja+RKopviAkVyrH8aH719G8AAGGRQWvlmVTspGerfYf3DqUK5ghY42w2Z4zX9nlHwmSw\nl0G2o6ymCdsh0GASbPPfd4HvtwlkFU0lglrHhWPgDg4ODhWKimPgx52zmve2idbxDSnLz7bM7Ue3\nrBnTW1yDomf3bvE/Youh2mn8zq3n8f7727Ytxr/mxoKRsEwI/0LJ+9trbMb+AwvGT5FaLrLsv9XV\nb+n+4gzRhrbFxHrqmy19aGhuAQAEa6z0EQpxxKRIDhNtof22WmLUXcPCZ4vx8P22tNt555wLAIgE\nrSXKRCtufamYNT/3ArnZfeiic2yjXyILZjR33kaugs9s2lh8TJmI9xOTlW+VIVshEVZnghUtU5cM\nnOYYFhGc2V0k4aTYdRAAsiwWGE/BRiHssbckDqSt7BCpoRTA1Y3WlbObc70MJvi4vLgmv2OZlDBi\nsnFUFm0wx+X5rgOiIEWaqWHIJ3wwnxs/jnHzr6zbZl00yvO373VigNwee7dZh4SWea08Li3oyKj9\n2+jpoAIeqTctAz/v/PNprFOtpLjuIYq4jbN0aJwXAGDlmZ8FAASTNq1yopPWJR23NN4sWz8XPTlr\nmX3/murp77Gr2xrbswla76F77dpGWJIbDdHf1dDCFV7faISe22jWrkEg7PNeTwDHwB0cHBwqFO4D\n7uDg4FChqBgVyvW3kEicfOrGso5vX/pRAEBqRGS+KajDCEBWoGeLYn9vHGMRbvQRbXrJrxrdL9o2\nY9A5V4j7y1j9kf6obQvxdU1RzhphDWkmMU66rn6guQ7TRp58efcMWpG3q5/Et7Y2nkerFUNjAZpB\nLi8SKeWM6kTEvXG18zvuocRYV175V15X515SHy0/c5nX1tZ8PE1HiN61XE7lwgsvAgDcfPe9Xt/y\nc8nAJdOh+pX+jm8zYjWrAEJ+OYDLQ//AuNmRESqI28sVbPLC6TvAKpRI1qqseh6nd1j6/jaZyEOe\n7pDQ6tXxbR5TZ9VNS0+nhGzdoqrPqz0sqrM0ns3aezfanawQ1bPGQCcmkvfCAPkE4fPtJYDy4Xul\nVCiRyHxvP23SyabtvZw4h9QlqYT9m0u+wYbyo0ldV5eTKhd6n7p2v+61ffoLnwQA1AqjbmszvU+X\nfuw8AMDOTpsI7YmnyTsgmbQPuqmR3mFZr6CWqxXNn0eWZGkI7dzyPADgnkesMXppNf2dr+i26p0m\n/qY8kyNVyy+et6qi7nr6+5J1Sc378ykb8jAhHAN3cHBwqFDMaga+cZutcH7TdZMruXnFxy4GADx2\n/32i1fz60691zbHWaHcUp0i94MKx9ZuB2uraojbsZtYwZH9xUeMT8mWIwfkfKu4rE1PI814Ew9xy\nSctoqmqZ2Tf6uCfGmBkMCue7NBuUCsgtMbsGLuhww7V/6fUMJ4l9XnaJfab15pqyKCAbuFYso0IE\na/7rP7yu004/GQAQidhXNcvpXnOjlnV17SYDV209tbW0NXl9Wy0BKwvDvKa5fDG7lAzcRGcaF8Ng\nwM4nzEw23rnZzpuHO6XBjhetp3PjfXR8q1iKiz9MKXqXLGn32iIROvm5zh6vrf83tEb5IC1MLieq\nx/NW3osxEGZFTtXcmGjSoGDWOZPaVbD+HK9fKQbeP2xfFGPobRBW2qUnkM9swzx70/E+YqmhEEmi\nw/1Was4MUf6c4VFrKH/syQcAAPNb/9xr+/SV5BZrVuoMUewjlWBJ7m27jpEmlgREzc/LLqckrN//\n1t8CAJrqjvf6Rry8y/b40Of/FQDwme9f67U9t4G+Xz9+ei0A4Bd91k03t5v/luTfgUkU4xi4g4OD\nwzsfs5KBD7LacOcO+4v1z9/+ZwBAkpPcp4VrVQ3/0J+2yBYruOhcqsh+w3Wf9dqeevoFAMCDD1MB\noU983FZXX3b6HxbNI52jXA01QR9mfbbJhDd+RrzDjpzVew52UJ6RxYuWeG2RxpqiUzzUkNQRFPky\nAjnjRyZlAtqPsGtfW7Nlvq2txBzrW0V0A5epQ7XQ6/O1IsyZTlm4wI6eN1XbLZsbjNPa93Zb3ekQ\ns/26errWV1f/g9d3xWc+TmP02qCQ+G5iesWOpIWFDgxM0EZIuAqOZeCRkOVDmQEK0AkPWqZ3Sgtt\nRXJGbNlFDOysM2neX7rGSmpN9WRrGBza57V1j9Lxz3RbFprKcPEITuGX9XWAFHl/WN9dyMrNffq4\nRDLPyxW4J3LgTwkGfkA8j0CY9lPCDTPGuXW+/NlzMB66tlkJ9/ob/p7m0WuZ76M/fxgAcNE5doxF\nrWcCsMJvKikC/N42fxPCrdLYdMStROfQ37X5ttTWWSl8JG7eI/sO33c/zeO+PiFeDfDaR3nBg0KS\nrynlMlhuyFkZDFwpNVcptV4ptVMptUMp9UVur1NKPaGU6uKtj57BwcHBweFQoRwVytsArtNazwdw\nBoC/VErNB3A9gHVa6zYA6/j/Dg4ODg4zhHJKqvWD/e+01iNKqVcANAG4GFQrEwDWAHgKwOQsjQLr\nnrciXojLM68616o1YszvJ1u5WR6+8uz3AwBaYqQSaT3hfSXP7dpJ1q9I+PCmk50yglbt0GCyj9aV\nUJsUgMXDahsJFzJhg2khonMq0/qmJt7OtX1s2MxlrEgYrGZxUpYWN75udWRgbYod43XFe8lgOdBv\nx2iMkiEsk7brkmEXsARHI85tty5sl338D2hHRKF+/VpKpYt6od4xY2WKQw5NTpO8iFA0beYpZ0SV\n93rebxDqEqM6aWi2azC8k+a0j8X8phY7n1Eu0DCcsjxre5IsaNsG7LoY9YcxTuaCUoVSrOrwM0Da\nXe4T+oSAUZ3IOpJ8QrZEFYJ0QWQqbzJWJXJMbOK0wG0LrZriq/+P1GJL1ln3vRTneKmuEgbt3rd4\numzQbrdG4J5ujozeL1xF40ZVaw2m6+7nAhvXkwE0Grb30n4ijVctjP+9KXoOH/qQvadInj5avbvo\nO9KxS9QqjdA7nC0oEjr578ykjJhKqRYApwDYDKCRP+4AsBeAb6YlpdTVSqkOpVSH9LN0cHBwcJge\nyjZiKqWqAdwL4G+01vuVsknYtdZaKaX9ztNa3wrgVgA47rjjio656bafAwBkCo1PXE75DWqFVn2y\nzLsUJmLeBgsXkiGjp0/m1XiTt4Z3lctoJ4cekRi+ZeoxKRZ1x07yBH7gUbsIQc/tzGcxmD1DZL0D\nBz8ERQGGkUH6zc+kPF8shJNkRKrhPBl1UfFMDS3IWENeYoDYbSIhDHkpYkjGXjQsjK8jA5Sxbuu9\nD3ttHeupTN7JCy1TNzBESNjsEGA2mRUlxLJcYSDK2f1CO21ujAYWIiJijBM5wOR1IU30mmtupfnu\nHbYM1fCdkZBdg009VPIvWcCFCnmY9EzL5QuDjQAgz+sn7Y/eOZ4N066xt+p+4wZLsEbhVgkOJAoI\n18XhfmOELu/vcRHnBFq04Jqyjjd4dv2D3n5vH70z+wbtuzOS3A8A2CPyIJlZdu2gNf2jj13k9S0+\nnRwBwuI9zXNQXrjGSlDf/3fKvfTY/U9QQ0o8wDMv4R2xdlP4yJXFwJVSIdDH+ydaa+NYPaCUinF/\nDMD44WsODg4ODgcd5XihKAC3A3hFa/2voushAFfx/lUAHhx7roODg4PDoUM5KpSzAFwJYJtS6iVu\n+78AvgHgHqXU5wC8AeATU5nA9V8orgp++/coanDlh1d4bav/lHIetJ998lQuMy1UV8nH9J4pjdE5\nYMWnFzsoFW58C0UPbvqlTbu5jXOx7Op+FNNFLmErswfrp6iDkva8KhYZq3w8Ro34J3JuIMIidMKq\nMzJsmMuLPBz72YgZytfz8NaXds/uHgDAzq29Xpt5klkRFWkMVib9qKm8DgB746SW2LnNRkXGh8l4\n6Pc2mQITvh7OIj1sNRdLqOUI1eCRYgx+VHMXWpE6PkL3/nhx1lxPEff4k894bfVcIKGq3eaS6ejb\nTteSehLjp23+JyfuaVBkYYl8QR8glWI8VkGG5vyYLWCMo6FSFFCsO0Imx4q90qYn1wEANtjMuFh+\nri3AcrAgbaWxKOvkFviZ7Jb5tBFu/Jd/LOtaP3rAGljvvu0fxj8wxyoZ8T4hPXkjZjleKBsBqHG6\nV076ig4ODg4OBwWzMhIzsZeyfd295nmvrXcNFTp4Vr/le86hRGbIGitSUWJzvQNkRdq60xpDtm8h\ndtTxrGV6W16iBO+De2zbTCEvCi8EMdmiEGxvFkw5N0SsNRj1YS8jbFWTZdiZoeaG93tNAXa3SiSs\ny9bePpI6sgsoCURLxPreNdSTG1lHykblmuruwSqZMY+NjFykYGDAjn8E5/DYK8pz+dQm8OAZKiXh\n5DwVsjB7NElrn2eXyKwoxtDNhtt00hry2k8ghhftetpri7Ad79jj6bidnXbei6NETV/tt1JNPxvC\nCgpLeJUl+P8FkZgcpSnaPKYub9AsW4CzOcKnSxo9vUaMDxnRygbt4YxlmV1cou/uu6175zPryWFg\nfivd+/wF1gWwrZ19YUPCOBp5d4kJzDyi4fGLy8gsmi1z6P5SaftQMxmzzrLGYmm4XCgODg4OFQr3\nAXdwcHCoUMxKFYoR2mqE2Jc3BiuuUYdYsRj/8APWGNgUI7/nxaf/wbRn8+fXfM3b37KT7LjxQePD\nmvY5Y3YgVDeNWpr86NMJ65e8Z4DSec4VhqiqOXwNU+tQUgI2JAYjNrKyLkoHvLrD1jTt3NYBABgY\nJhVHQ5M1YsZOpZiAlSIacdPGTTxHacQ01jravrrN1ibMNJOY2tVpDaF2rwSE+qiK37+AiLZMDpPa\nKMI23WyVqGoeoP3RpH1We1+he8jGbLKu1jgVGMhzUcxUwEYeDiXoXjp2rLPzYENXTqgRAmxJNEUZ\n5BKI1Fvenrd8BX7HpUR/c1zfwGgAABRgSURBVLg0svHIgRLnCVWYyeQcEiqUIDvtB+vbvLYkz6lr\nmLaJl6zaa+MWemdkBGmIn0Os0b4zteyfn0zR+9fbL2pX8rY+ai2b5rmlRQRuhP26jWouJQyMw1xc\npE/EIQwO0Xvxo1tvx/iwY/RwdGaoys4jWNIi7A/HwB0cHBwqFLOUgdOvulTli4JQRUcfxQxv5MBQ\nUd9/f/f73v7Vf/35Kc3m4afunfigdwJGhDSRoqe/p3eP19TPeUbicctfV6wyZeGMe6VgZJFC4xoA\nZLlmWE5Ulk9m6JxENxnwBvttktcWztbbcvaZdmpcDquryxo2wybqjxOOJAWbCrFRMicmMnEWDsvu\nAHiFO6r32xDIqmPZdZGj7w6ErLE7bLwqBUca4ojNqpBlpumT5nAnsbq4qPI3uqMHANAQsuuyqIYs\npV2i1Fgun+FrGRRbG/MTpJjltC7IGxfDgijU4r2g2Q+W4IBVoo/Zez5in2ma0wh3jVhjezRL/d0J\nelbZpH3XgiP0foSrbEm/EOdAWSjynSxqI2P4zi6KrNzyspX2jPG1WuT0HfwNSZaDIjrz+FYaI8nF\nQ5IishdcPf7VHisdDHoG24klGQBAF52bFcVissYBYFm13xm+cAzcwcHBoULhPuAODg4OFYpZqkIp\nRlM1iac5FsG+cO2XvT4/1YnBNV/8grc/VRXKoUZUVNQOhetLHHmIkGIRXYiQvd1U+XtIGGrMz/1Q\nwhryhrpeBQDULTYqFGEYY5VBdsCOEecam09ttuGI9/+MfH8jbNSqabDJw764gOpkov69Xtv8j1KK\nz/n9J3ltIwOkVtnOomksZisDhVlh8sNR62NdKubNJKBKvW6TG5nj5zZY5UuGDY8H2L/XpJcFgIA5\nQziOexV8hIoj1ciGT65Vms9aFU0qT+Mmtlk1Ql097Z/Suthr62Gp3RgAs0I9ZWpRSj/wgNkXPvtB\n60TuA/YNLwhN5XNLGTGFjzNGSM2Tq7HqAdaSYHiHvT/z3IL8HEJZq06LsFK1vs6uQZbXfXDIRt4+\n/ksycmfYCF0dtdfMcgTrgDh+f5LaahusMXX+qVThJ1pN9zk4aOc4wgbNhWes8Npq68mp4s6f2Bq8\nXdt/jkIIpwKjY0vZGAn7UjoVioODg8M7HhXDwOMB+lX62Gf/DADw4H0PHPJrbtvxysQHCay6iGov\n3rD6617bssVkhesfshGkxx0TLjivVkQ2/tO//AdmDKYwQx8x02S/Zcr9feR6dSAr08MSs6sR7mG/\nepqiCi/mgg5oFCxjhBhTVrjjrd9IEalfue2O4vnsI9a18Zu3eE2nnbwUALD0k1cWHx+zrLyG95ee\n7JPdpI/m8e3/trnY1j70jeLjGPnMaFGbSYeTqrdufnlm1NWGjAqKWhXiNZaRqewmFgpY/l/F9Riz\n7F452N3l9W1mai2c8fAXJ9AatL7HXqv2LWrbw8y7J2XHN0JBUNS/tJKCcNMN0jxynitbcU3MYEH+\nWWPsHL8mJjKSnbNUMGQljOERfs6CZQcjlFDmxFYy7o4O23vp5TS88YRlw8dygRJTFxQARtgYH2VX\nwWjESrWhd9E89gmpMMmG022cmwgA2t5PRvPmthMBAM9ssVHhW18iqTMUtCtzbAMbMbOSPZu/Bb6H\nI23Sl+Ac4/ZopaVccvL1EhwDd3BwcKhQzEoGHjqCXHiahGvQV79I+usXd1MgzYMoZuDnLbV6wSc2\nbSnqH+ykX92GdmK88b7feX0Pc6X6hx8SSf+30BhNLZbVfek6Cur5q2svwWQQq3uXt//t/7oTAHDd\nn10BAKiptkEIn/rk5MadNEbsPadYXzyaIAaUFhWTBuOcn0ScGmFGk0xZl6p4Xw8AYMNjVCBh+WWX\niRO4pJrIDLgvWcxuS2ELB28sPec121jNbmc1MitiiZwYc4ilLfz4VV7Two+vAgCsXn1L0eHV9TTv\n6lorKUVqqE2WwDKui1HmQRHBtnM5klzyITtGjt3qgoJ1RQyDHaY1MKxbQvKySD2tQVu7fWdOqqb3\nuYvzpDy4xbL4US+3iXTpK85MadY54Om2bV9A5H8sPqOENSFpmbVXnSIspE+uzB4UlVuOYhtXnnXV\nCZHRMGX8ioWEkeWgmrki+KuGS+7VmOAoIQkk4gM8ri0QYsrCyepwT/1yI1+TWL+sU9Iyj1h5UujR\nu7p6AACDCXHPMPdlpAP7UHNJstuFakVJPy+4qExXRDgG7uDg4FCxcB9wBwcHhwrFhCoUpVQVgA0A\nwnz8Wq3115RS8wDcBeAYAC8AuFLrqeR6NeKkEIveJtG+Z7t1V0uMkIEwOTy+CO6nNpFo/ANTF5LE\n4cVLzvH6vrqaVCOP/PzOsmY9HfzdNZcXbA8ZUsK9coANiikrCw4OUJTZkBH7hD0qxQbOlHBr6+cU\nrfGBuNdW9W56ls9spHwm+ZxVD5xxJhkg9wu1SUtL66RuYc8bdK07b7HG3WiYeEdtnRU/W9spv0gD\nG51QJ3LlmDmlhEtk4/iFRqujhtdYcT/I6pKwUKFEOZqvlVODnrXI1tccHqZntfElGwW4n1/xnKBN\n1XkyuPX3l5WdBT97mP4mYnV23KVLybA6j9U8i+dYo10Hr9mI0JqY6vUFKWNZrRLy3AKLizfkC/L+\n8M2UrIkpo3J5DcIiupWvkRu2aodhdukb7mbViVS5mSKlIp9KmtUpw9W2rZaX1ktdnLYx3Sku+JGV\nU+O6r7HmFq+tig32Xbvp+WUzdowIpwpumGPdDmsb6b2u2mWjM7eN8hqZiN5osYtwdlBUovT+dnwK\npoyDchh4BsAHtdaLQAVMVimlzgBwE4DvaK3fB2AYwOfKvqqDg4ODw7RRTkUeDcD8DIb4nwbwQQCf\n5PY1AFYDKLYITTg+kfYtnW94bR3PE5t76nGbhW2QDWdr770PY9FQS79sg8OJoj4/XHQxMd+HHiiV\nOaxy0Xkv5W7JCLYd4OxqyVHLaA6wYSnF24AwrmWzxMj2C4ln5w4KvhlI2OxusRZylQpwTow777Du\ngRs3kCFoyeKFXlu0htjLFRdb6ePOB++ivgC5jq0Sx9+0lsY7b6512Vp6KvXX1FhG09lLzKetl/oa\nY6JOV47uJSoCOo7sHz9YoobZYiRgr3kMu3rG6iw7ioSI6iW5knx1laV1Ky5cDgBoaWux9/kIGXpH\nhVtlmN/rRF95WS0Nz4y/YVlrchGXdqune1q+6ESvL9fJHC1i87RE2Lg8PGwltO4hNmjnaT4FxR5M\nnhQxDy+vTLCEG2H+bbufZklOSHTgwK0C66HpN8fLXCse8y52cUwKA3wobVIfchCTyGkTZrYdqRXZ\nGdl1MiWYvdkbYlfHbEZkY8pSWyZtJdHqKnr2oYAQdaLscmoyGUYFsw6b9RBG3VJVRsZBuVXpg1wP\ncxDAEwC6AfxWa21WqA9A0zjnXq2U6lBKdaRSk/dzdHBwcHDwR1kfcK11Tmt9MoA5AE4D0D7BKfLc\nW7XWS7TWSyIyP7CDg4ODw7QwKT9wrfVvlVLrASwFcLRS6ghm4XMAxEufXRqL299btH/1lR8rPm7x\nIgDAhRes8Nr2lVCdXHSxjeB76IEfTWeKsxpb11jt1bpfkOoplREST5jEt5o66y9rJKLaOhL1Aln7\nex4yBrcRkZuDU56GhAFoOM5pMVkMllJW5/OkQul52UZnLvkAqRbO+sMWMXta52QvGYzu6xibQwJ4\nYo8dd16MrznHirxJrgw/zEn8axt6vL4I+2K3t9tCCvX14xuKIhEat6V+jtdWGyWDaVPUlp6P97Cv\n/Fv0XOS917DKZfmZVpWT5mjB5zZv8tp6dlFBh0GhbSgFc4XOLivSn8SpTk1kYFOzFYZPj5Ga6e6f\nPua1NXC1+8Wn27iJtgZSB3VzUY1kyhrtDnBOk4R4n0JsSB6pEWlWx0IYD2HUeSIVrNcmjxtl1VA1\nHyeKdsBEsNZaIljFvt7HNoiiIWzATnDOnsSA/T6YNaqJWpWSUf/JQhHVnLMlwRGbVUINk05x/dWk\nXYNE0Ki0xHxN2mCOcsWQyHtST887ItQqBX+vZWJCBq6Ueo9S6mjePxLAeQBeAbAewKV82FUAHpz0\n1R0cHBwcpoxyGHgMwBqlVBD0wb9Ha/0zpdROAHcppb4O4EUAM2IRvOD8s4vaDCFsOtL+qg5z9NU7\nmXVLPPyQjUzd8PQGAJYVA8D+DDGI49utgdCotMxxVcKgciy7PFWFrVHGsJKI8INLcoa/kSEZgUYY\nZb+5F1/c5bVt4v28oA5xJj7l1uK+8zliVG2dllkdVUeMJsjltKL1VtJoipEEsGfQHn/SQvscxqKW\n2VZaSB+jIdqvb2/x2lrnUZ6blhixwJ3brGvfD2/5MQAgMWSvWces/5Q2626Y3EUZD5cfT89q/qk2\n6jfA5bYa37Iiz6YOkq52dtm27a/QM02+h4yS84X73lGcoTAzZF0ov7OB9k98ZLPX9qkrKffHyg+R\n62dWZFbs7aGCB7/a8pLXNpSm+9ofEeLYWDqYkXl0uFMaLFmiQ16ybD7OFFyIFvg/AgCC1ZaBm2IW\no6MHvLbkKLlkDveyUmBESAlv0zqOJIQ7I18rJN6Z2jpi9K3zWotuzdg6A6IwxyjfS3+/UESMMkMP\ns1+jMISavnxElJiLGNfW8iMxy/FCeRnAKT7tr4P04Q4ODg4OhwEuEtPBwcGhQjErk1mVg2/fdLO3\nf92X/gIAED9ghfBv3jSDaVlnAZ7bYVNhPrHPz6eYxLItW21azOLKiBZGOFwkku3Emji6MW/Hj7OB\nqJfVJTLOzqyGbKtnytAQs2lZT2omQ58x6FQLb6XRBEWq9ffZCLfRJI0cFsc1c4RnbSMbHoV4ayqM\ny2IZWYzvEdW5ma41PGSv+ekvUCIsqXqZUzeXxk+R+P7Ylru9vsEhmuOGZ63BMsr3ueqPLvDaVpz/\nx3R8L6kp4qLq/fIlZLCvFaqttexbv1OkuBpcSyqDi06hbV4UkYhxkqcVZ1vVTG0jqW2y1Vbl2Pcm\nqQBaBmgd206wEbMN7fRMq7JHeW3P9D4HAKhptWNgbDCpUMMgZN428TYYo11IqEmM6iTstz4cuZm0\nqpmcicSss29xzkSMGrWNKAABTh0bjAiDPV+rusZeM2yKY7A/ekIUMUka46tMl2s0IWlxL2+P8VuP\niNiDEXpn0lmhcvHe54Mbieng4ODgMAuhKNByZnDcccfpq6++esau5+Dg4PBOwI033viC1nrJ2HbH\nwB0cHBwqFO4D7uDg4FChcB9wBwcHhwqF+4A7ODg4VChm1IiplHoTwO8AlJf3dfaiHpV9D5U+f6Dy\n76HS5w9U/j1U0vzfq7V+z9jGGf2AA4BSqsPPmlpJqPR7qPT5A5V/D5U+f6Dy76HS5w84FYqDg4ND\nxcJ9wB0cHBwqFIfjA37rYbjmwUal30Olzx+o/Huo9PkDlX8PlT7/mdeBOzg4ODgcHDgVioODg0OF\nYkY/4EqpVUqpV5VSrymlrp/Ja08FSqm5Sqn1SqmdSqkdSqkvcnudUuoJpVQXb8tPH3YYwEWpX1RK\n/Yz/P08ptZnX4W6l1LsO9xxLQSl1tFJqrVKqUyn1ilJqaQWuwd/yO7RdKXWnUqpqNq+DUuoHSqlB\npdR20eb7zBXh3/k+XlZKLR5/5JnDOPfwTX6PXlZK3W+qjXHfl/keXlVKffjwzHpymLEPOFf0+U8A\n5wOYD+AKpdT80mcddrwN4Dqt9XwAZwD4S57z9QDWaa3bAKzj/89mfBFUBs/gJgDf0Vq/D8AwgM8d\nllmVj+8CeExr3Q5gEeheKmYNlFJNAP4awBKt9UmgTL6XY3avw/8AWDWmbbxnfj6ANv53NYBbMDvw\nPyi+hycAnKS1/kMAuwB8GQD47/pyAAv4nJv5mzWrMZMM/DQAr2mtX9davwXgLgAXz+D1Jw2tdb/W\negvvj4A+HE2gea/hw9YAuOTwzHBiKKXmALgQwG38fwXggwDW8iGzff5RAMvBJfu01m9prX+LCloD\nxhEAjlRKHQFKt96PWbwOWusNAIbGNI/3zC8G8CNN+DWo4HkMhxl+96C1fpwLsQPAr0EF2QG6h7u0\n1hmt9W4Ar6ECKo7N5Ae8CcAe8f8+bqsIKKVaQKXlNgNo1Fr3c9deAI2HaVrl4N8A/B94WeVxDIDf\nipd4tq/DPABvAvghq4FuU0q9GxW0BlrrOIBvgUoe9ANIAngBlbUOwPjPvFL/tj8L4Oe8X5H34IyY\nZUApVQ3gXgB/o7XeL/s0ufHMSlcepdRHAAxqrV843HOZBo4AsBjALVrrU0CpGArUJbN5DQCAdcUX\ng36MjgPwbhSL9hWF2f7MJ4JS6gaQivQnh3su08FMfsDjAOaK/8/htlkNpVQI9PH+idb6Pm4eMCIi\nbwcP1/wmwFkAPqqU6gGprD4I0icfzaI8MPvXoQ9An9balFBfC/qgV8oaAMC5AHZrrd/UWmcB3Ada\nm0paB2D8Z15Rf9tKqc8A+AiAP9HWj7qi7sFgJj/gzwNoY8v7u0AGg4dm8PqTBuuLbwfwitb6X0XX\nQwCu4v2rADw403MrB1rrL2ut52itW0DP+5da6z8BsB7ApXzYrJ0/AGit9wLYo5Q6kZtWAtiJClkD\nRi+AM5RSEX6nzD1UzDowxnvmDwH4NHujnAEgKVQtswpKqVUgleJHtdYp0fUQgMuVUmGl1DyQQfa5\nwzHHSUFrPWP/AFwAsvx2A7hhJq89xfkuA4mJLwN4if9dANIjrwPQBeBJAHWHe65l3MsKAD/j/eNB\nL+drAH4KIHy45zfB3E8G0MHr8ACo6mtFrQGAGwF0AtgO4H8BhGfzOgC4E6Svz4KkoM+N98wBKJCH\nWTeAbSBvm9l6D6+BdN3m7/m/xPE38D28CuD8wz3/cv65SEwHBweHCoUzYjo4ODhUKNwH3MHBwaFC\n4T7gDg4ODhUK9wF3cHBwqFC4D7iDg4NDhcJ9wB0cHBwqFO4D7uDg4FChcB9wBwcHhwrF/wefu+0e\nHPIW6AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "truck   dog  bird  ship\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_xGcJcrTgt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqoesEtFbh0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g17zlo6cYOw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "00537e92-7ffe-42ca-8d42-2e31ed489cd1"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mruZ8tbGcoa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config:  \n",
        "  def __init__(self, **kwargs):\n",
        "    for key, value in kwargs.items():\n",
        "      setattr(self, key, value)\n",
        "\n",
        "\n",
        "model_config = Config(\n",
        "    cuda = True if torch.cuda.is_available() else False,\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    seed = 2,\n",
        "    lr = 0.01,\n",
        "    epochs = 4,\n",
        "    save_model = False,\n",
        "    batch_size = 32,\n",
        "    log_interval = 100\n",
        ")\n",
        "    \n",
        "class Trainer:\n",
        "  \n",
        "  def __init__(self, config, train_loader, test_loader):\n",
        "    \n",
        "    self.cuda = config.cuda\n",
        "    self.device = config.device\n",
        "    self.seed = config.seed\n",
        "    self.lr = config.lr\n",
        "    self.epochs = config.epochs\n",
        "    self.save_model = config.save_model\n",
        "    self.batch_size = config.batch_size\n",
        "    self.log_interval = config.log_interval\n",
        "    \n",
        "    self.globaliter = 0\n",
        "    self.tb = TensorBoardColab()\n",
        "    \n",
        "    torch.manual_seed(self.seed)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if self.cuda else {}\n",
        "\n",
        "    self.train_loader = train_loader\n",
        "\n",
        "    self.test_loader = test_loader\n",
        "\n",
        "\n",
        "    self.model = Net().to(self.device)\n",
        "    self.optimizer = config.optimizer(self.model.parameters(), lr=self.lr)\n",
        "      \n",
        "\n",
        "  def change_conf(self, config):\n",
        "    self.cuda = config.cuda\n",
        "    self.device = config.device\n",
        "    self.seed = config.seed\n",
        "    self.lr = config.lr\n",
        "    self.epochs = config.epochs\n",
        "    self.save_model = config.save_model\n",
        "    self.batch_size = config.batch_size\n",
        "    self.log_interval = config.log_interval\n",
        "    self.optimizer = config.optimizer(self.model.parameters(), lr=self.lr)\n",
        "      \n",
        "  def train(self, epoch):\n",
        "  \n",
        "    self.model.train()\n",
        "    for batch_idx, (data, target) in enumerate(self.train_loader):\n",
        "      \n",
        "      self.globaliter += 1\n",
        "      data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      predictions = self.model(data)\n",
        "\n",
        "      \n",
        "      loss = criterion(predictions, target)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      if batch_idx % self.log_interval == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, batch_idx * len(data), len(self.train_loader.dataset),\n",
        "                  100. * batch_idx / len(self.train_loader), loss.item()))\n",
        "        self.tb.save_value('Train Loss', 'train_loss', self.globaliter, loss.item())\n",
        "\n",
        "\n",
        "  def test(self, epoch):\n",
        "    self.model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data, target in self.test_loader:\n",
        "        data, target = data.to(self.device), target.to(self.device)\n",
        "        predictions = self.model(data)\n",
        "\n",
        "        test_loss += F.nll_loss(predictions, target, reduction='sum').item()\n",
        "        prediction = predictions.argmax(dim=1, keepdim=True)\n",
        "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
        "\n",
        "      test_loss /= len(self.test_loader.dataset)\n",
        "      accuracy = 100. * correct / len(self.test_loader.dataset)\n",
        "\n",
        "      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "          test_loss, correct, len(self.test_loader.dataset), accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDP743mSwbp7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b5dde760-db7c-4a23-e3e7-79512de2c003"
      },
      "source": [
        "cifar_config = Config(\n",
        "    cuda = True if torch.cuda.is_available() else False,\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    seed = 2,\n",
        "    lr = 0.001,\n",
        "    epochs = 10,\n",
        "    save_model = False,\n",
        "    batch_size = 32,\n",
        "    log_interval = 1000,\n",
        "    optimizer = RAdam\n",
        ")\n",
        "cifar_trainer = Trainer(cifar_config, trainloader, testloader)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://2743a3c1.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FDOiaz_xLOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b389957-f5a3-4225-e93d-d253f64ad3fd"
      },
      "source": [
        "for epoch in range(1, cifar_trainer.epochs + 1):\n",
        "    cifar_trainer.train(epoch)\n",
        "    cifar_trainer.test(epoch)\n",
        "    cifar_trainer.tb.flush_line('train_loss')\n",
        "\n",
        "if (cifar_trainer.save_model):\n",
        "    torch.save(trainer.model.state_dict(),\"mnist_cnn.pt\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.283214\n",
            "Train Epoch: 1 [4000/50000 (8%)]\tLoss: 2.152855\n",
            "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 1.656289\n",
            "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 1.389385\n",
            "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1.793756\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 0.933540\n",
            "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 0.969553\n",
            "Train Epoch: 1 [28000/50000 (56%)]\tLoss: 1.910572\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.166584\n",
            "Train Epoch: 1 [36000/50000 (72%)]\tLoss: 0.769097\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 2.174813\n",
            "Train Epoch: 1 [44000/50000 (88%)]\tLoss: 0.800112\n",
            "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 0.621000\n",
            "\n",
            "Test set: Average loss: -1.9717, Accuracy: 5281/10000 (53%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.744307\n",
            "Train Epoch: 2 [4000/50000 (8%)]\tLoss: 2.108772\n",
            "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 0.428846\n",
            "Train Epoch: 2 [12000/50000 (24%)]\tLoss: 0.816916\n",
            "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.628302\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 0.625409\n",
            "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 0.892590\n",
            "Train Epoch: 2 [28000/50000 (56%)]\tLoss: 1.150427\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.247566\n",
            "Train Epoch: 2 [36000/50000 (72%)]\tLoss: 1.356715\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 0.826772\n",
            "Train Epoch: 2 [44000/50000 (88%)]\tLoss: 0.901072\n",
            "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.472459\n",
            "\n",
            "Test set: Average loss: -1.8716, Accuracy: 5675/10000 (57%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 2.132877\n",
            "Train Epoch: 3 [4000/50000 (8%)]\tLoss: 0.742003\n",
            "Train Epoch: 3 [8000/50000 (16%)]\tLoss: 1.186518\n",
            "Train Epoch: 3 [12000/50000 (24%)]\tLoss: 2.171916\n",
            "Train Epoch: 3 [16000/50000 (32%)]\tLoss: 1.695178\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 1.877293\n",
            "Train Epoch: 3 [24000/50000 (48%)]\tLoss: 1.939331\n",
            "Train Epoch: 3 [28000/50000 (56%)]\tLoss: 1.674273\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.295754\n",
            "Train Epoch: 3 [36000/50000 (72%)]\tLoss: 1.717145\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 0.632985\n",
            "Train Epoch: 3 [44000/50000 (88%)]\tLoss: 0.907202\n",
            "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1.549593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-25c6f6df2067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcifar_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcifar_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcifar_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-80adaff9012e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-43ee5c557295>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkfkx-wHdES_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}